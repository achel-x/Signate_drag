{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019/09/12 ver.1.0\n",
    "## 一、文件内容\n",
    "从文件引入数据<br />\n",
    "从文件引入模型<br />\n",
    " 1：支持向量回归机<br />\n",
    " 2：随机森林回归<br />\n",
    "## 二、数据集\n",
    "处理后的原训练集 特征数据：dg_pca_train<br />\n",
    "处理后的原训练集合 得分数据：score_train<br />\n",
    "处理后的测试集合：dg_pca_test<br />\n",
    "测试集序号：id_test<br />\n",
    "\n",
    "在dg_pca_train中继续划分数据<br />\n",
    "X_train：0.8比例的原训练集特征 用作训练和验证<br />\n",
    "X_test：0.2比例的原训练集特征 用作测试（模型选择）<br />\n",
    "y_train：0.8比例的原训练集分数 对应X_train<br />\n",
    "y_test：0.2比例的原训练集分数 对应X_test<br />\n",
    "## 三、单模型信息\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文件名|变量名|模型名|超参数设置|训练集上预测值|最终结果|是否导入\n",
    ":--------------:|:---------:|:------------------:|:-----------------:|:-------------------------:|:----------------:|:----:|\n",
    "SVR.pickle|svc|支持向量回归|C=10 gamma=0.001|0.5221925320557665|0.5311109175460105|\n",
    "svc_g.pickle|svc_g|支持向量回归|C=100 gamma=0.001|0.5234655148127494|0.5335009474194661|\n",
    "svc_best.pickle|svc_best|支持向量回归|C=10,gamma=0.0001|0.556846220888782|0.5533334949038184|√\n",
    "svcRF.pickle|svcRF|随机森林回归|n_estimators=300|0.3619639542577069|NA|\n",
    "svcRF_best.pickle|svcRF_best|随机森林回归|n_estimators=500|0.36527243324366043|NA|√\n",
    "xgbr.pickle|xgbr|Xgboost回归|n_estimators=800|0.4505720493043171|NA|\n",
    "xgbr_1100.pickle|xgbr_1100|Xgboost回归|n_estimators=1100|0.4600838333957028|NA|√"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、交叉验证模型信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文件名|变量名|模型名|参数范围\n",
    ":-------:|:-----:|:-----:|:-----:\n",
    "cv_svc.pickle|NA|支持向量回归|\"C\": [1e0, 1e1, 1e2, 1e3],<br>\"gamma\": np.logspace(-4, 0, 5)\n",
    "cv_svcRF.pickle|NA|随机森林回归|\"n_estimators\":[20,50,100,150,200,260,300,500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下为预设导入部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预设导入\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#机器学习导入\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import explained_variance_score \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取本地数据集\n",
    "dg_pca_train=pd.read_csv('dg_pca_train.csv').values\n",
    "dg_pca_test=pd.read_csv('dg_pca_test.csv').values\n",
    "\n",
    "id_test=pd.read_csv('id_test.csv').values\n",
    "id_train=pd.read_csv('id_train.csv').values\n",
    "score_train=pd.read_csv('score_train.csv').values\n",
    "id_train=id_train.flatten()\n",
    "id_test=id_test.flatten()#ID二维数组转一维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:30:09] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "#载入模型\n",
    "#svc = joblib.load('SVR.pickle')\n",
    "#svcRF = joblib.load('svcRF.pickle')\n",
    "svc_best = joblib.load('svc_best.pickle')\n",
    "svcRF_best = joblib.load('svcRF_best.pickle')\n",
    "xgbr_1100=joblib.load('xgbr_1100.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #如有需要可以载入的参数选择模型\n",
    "# cv_svc = joblib.load('cv_svc.pickle')\n",
    "# cv_svcRF = joblib.load('cv_svcRF.pickle')\n",
    "# #cv_xgbr = joblib.load('cv_xgbr.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10984, 941), (10984, 1), (2747, 941), (2747, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#划分训练数据集合\n",
    "#80%的训练集合 20%的模型测试集合 随机种子24\n",
    "X_train, X_test, y_train, y_test = train_test_split(dg_pca_train, score_train, test_size=0.2, random_state=24)\n",
    "X_train.shape,y_train.shape,X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下进行模型进一步的分析工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.556846220888782\n"
     ]
    }
   ],
   "source": [
    "#svm回归模型 C=10,gamma=0.0001\n",
    "svc_best=svm.SVR(kernel='rbf',C=10,gamma=0.0001)\n",
    "svc_best.fit(X_train,y_train)\n",
    "\n",
    "print(svc_best.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cv_svc.pickle']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#保存模型\n",
    "joblib.dump(clf, 'cv_svc.pickle')#C=10,gamma=0.0001\n",
    "#载入模型\n",
    "svc_best = joblib.load('svc_best.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_best = joblib.load('svc_best.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.556846220888782\n"
     ]
    }
   ],
   "source": [
    "print(svc_best.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:09:14] WARNING: d:\\build\\xgboost\\xgboost-0.90.git\\src\\objective\\regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xgbr=joblib.load('xgbr.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4505720493043171\n"
     ]
    }
   ],
   "source": [
    "print(xgbr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = [{'n_estimators': [700,800,900,1000,1100]}]\n",
    "\n",
    "scores_GBM=['r2']\n",
    "\n",
    "for score in scores_GBM:\n",
    "    print(score)\n",
    "    model = xgb.XGBRegressor()\n",
    "    optimized_GBM=GridSearchCV(estimator=model, param_grid=cv_params, scoring='r2', cv=3, verbose=1, n_jobs=-1)\n",
    "    optimized_GBM.fit(X_train,y_train)\n",
    "    \n",
    "    print(optimized_GBM.best_estimator_)\n",
    "    \n",
    "    means_GBM = optimized_GBM.cv_results_['mean_test_score']\n",
    "    stds_GBM=optimized_GBM.cv_results_['std_test_score']\n",
    "    params_GBM = optimized_GBM.cv_results_['params']\n",
    "    \n",
    "    for (param_GBM,mean_score_GBM,std_score_GBM) in zip(params_GBM,means_GBM,stds_GBM):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score_GBM,std_score_GBM,param_GBM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.556846220888782\n",
      "0.556846220888782\n"
     ]
    }
   ],
   "source": [
    "print(svc_best.score(X_test,y_test))\n",
    "\n",
    "y_pred = svc_best.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36527243324366043\n",
      "0.36527243324366043\n"
     ]
    }
   ],
   "source": [
    "print(svcRF_best.score(X_test,y_test))\n",
    "\n",
    "y_pred = svcRF_best.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4600838333957028\n",
      "0.4600838333957028\n"
     ]
    }
   ],
   "source": [
    "print(xgbr_1100.score(X_test,y_test))\n",
    "\n",
    "y_pred = xgbr_1100.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier [0]\n",
      "Training classifier [1]\n",
      "Training classifier [2]\n"
     ]
    }
   ],
   "source": [
    "clfs = [\n",
    "        RandomForestRegressor(),\n",
    "        svm.SVR(),\n",
    "        XGBRegressor()\n",
    "    ]\n",
    "for j, clf in enumerate(clfs):\n",
    "        print('Training classifier [%s]' % (j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# '''创建训练的数据集'''\n",
    "#data, target = make_blobs(n_samples=50000, centers=2, random_state=0, cluster_std=0.60)\n",
    " \n",
    "# '''模型融合中使用到的各个单模型'''\n",
    "clfs = [RandomForestRegressor(),\n",
    "        svm.SVR(),\n",
    "        XGBRegressor()]\n",
    " \n",
    "#'''切分一部分数据作为测试集'''\n",
    "X=dg_pca_train\n",
    "X_predict=dg_pca_test\n",
    "y=score_train\n",
    "#y_predict = \n",
    "\n",
    "\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))\n",
    "\n",
    "#'''5折stacking'''\n",
    "n_folds = 5\n",
    "skf = list(StratifiedKFold(y, n_folds,True,random_state=50))\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    #'''依次训练各个单模型'''\n",
    "    # print(j, clf)\n",
    "    dataset_blend_test_j = np.zeros((X_predict.shape[0], len(skf)))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        #'''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。'''\n",
    "        # print(\"Fold\", i)\n",
    "        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "        dataset_blend_train[test, j] = y_submission\n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]\n",
    "    #'''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_blend_test[:, j]))\n",
    "    \n",
    "# # clf = LogisticRegression()\n",
    "# clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "# clf.fit(dataset_blend_train, y)\n",
    "# y_submission = clf.predict_proba(dataset_blend_test)[:, 1]\n",
    " \n",
    "# print(\"Linear stretch of predictions to [0,1]\")\n",
    "# y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n",
    "# print(\"blend result\")\n",
    "# print(\"val auc Score: %f\" % (roc_auc_score(y_predict, y_submission)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_first(train, train_y, test):\n",
    "    savepath = './stack_op{}_dt{}_tfidf{}/'.format(args.option, args.data_type, args.tfidf)\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    count_kflod = 0\n",
    "    num_folds = 6\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=10)\n",
    "    # 测试集上的预测结果\n",
    "    predict = np.zeros((test.shape[0], config.n_class))\n",
    "    # k折交叉验证集的预测结果\n",
    "    oof_predict = np.zeros((train.shape[0], config.n_class))\n",
    "    scores = []\n",
    "    f1s = []\n",
    "\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        # 训练集划分为6折，每一折都要走一遍。那么第一个是5份的训练集索引，第二个是1份的测试集，此处为验证集是索引\n",
    "\n",
    "        kfold_X_train = {}\n",
    "        kfold_X_valid = {}\n",
    "\n",
    "        # 取数据的标签\n",
    "        y_train, y_test = train_y[train_index], train_y[test_index]\n",
    "        # 取数据\n",
    "        kfold_X_train, kfold_X_valid = train[train_index], train[test_index]\n",
    "\n",
    "        # 模型的前缀\n",
    "        model_prefix = savepath + 'DNN' + str(count_kflod)\n",
    "        if not os.path.exists(model_prefix):\n",
    "            os.mkdir(model_prefix)\n",
    "\n",
    "        M = 4  # number of snapshots\n",
    "        alpha_zero = 1e-3  # initial learning rate\n",
    "        snap_epoch = 16\n",
    "        snapshot = SnapshotCallbackBuilder(snap_epoch, M, alpha_zero)\n",
    "\n",
    "        # 使用训练集的size设定维度，fit一个模型出来\n",
    "        res_model = get_model(train)\n",
    "        res_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        # res_model.fit(train_x, train_y, batch_size=BATCH_SIZE, epochs=EPOCH, verbose=1,  class_weight=class_weight)\n",
    "        res_model.fit(kfold_X_train, y_train, batch_size=BATCH_SIZE, epochs=snap_epoch, verbose=1,\n",
    "                      validation_data=(kfold_X_valid, y_test),\n",
    "                      callbacks=snapshot.get_callbacks(model_save_place=model_prefix))\n",
    "\n",
    "        # 找到这个目录下所有已经训练好的深度学习模型，通过\".h5\"\n",
    "        evaluations = []\n",
    "        for i in os.listdir(model_prefix):\n",
    "            if '.h5' in i:\n",
    "                evaluations.append(i)\n",
    "\n",
    "        # 给测试集和当前的验证集开辟空间，就是当前折的数据预测结果构建出这么多的数据集[数据个数，类别]\n",
    "        preds1 = np.zeros((test.shape[0], config.n_class))\n",
    "        preds2 = np.zeros((len(kfold_X_valid), config.n_class))\n",
    "        # 遍历每一个模型，用他们分别预测当前折数的验证集和测试集，N个模型的结果求平均\n",
    "        for run, i in enumerate(evaluations):\n",
    "            res_model.load_weights(os.path.join(model_prefix, i))\n",
    "            preds1 += res_model.predict(test, verbose=1) / len(evaluations)\n",
    "            preds2 += res_model.predict(kfold_X_valid, batch_size=128) / len(evaluations)\n",
    "\n",
    "        # 测试集上预测结果的加权平均\n",
    "        predict += preds1 / num_folds\n",
    "        # 每一折的预测结果放到对应折上的测试集中，用来最后构建训练集\n",
    "        oof_predict[test_index] = preds2\n",
    "\n",
    "        # 计算精度和F1\n",
    "        accuracy = mb.cal_acc(oof_predict[test_index], np.argmax(y_test, axis=1))\n",
    "        f1 = mb.cal_f_alpha(oof_predict[test_index], np.argmax(y_test, axis=1), n_out=config.n_class)\n",
    "        print('the kflod cv is : ', str(accuracy))\n",
    "        print('the kflod f1 is : ', str(f1))\n",
    "        count_kflod += 1\n",
    "\n",
    "        # 模型融合的预测结果，存起来，用以以后求平均值\n",
    "        scores.append(accuracy)\n",
    "        f1s.append(f1)\n",
    "    # 指标均值，最为最后的预测结果\n",
    "    print('total scores is ', np.mean(scores))\n",
    "    print('total f1 is ', np.mean(f1s))\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2755 is out of bounds for axis 0 with size 2747",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-49e69d6432df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mkfold_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mkfold_Y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mkfold_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkfold_Y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#    print('训练集:{}'.format(train_index))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#    print('测试集:{}'.format(test_index))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2755 is out of bounds for axis 0 with size 2747"
     ]
    }
   ],
   "source": [
    "#Kfold 用法 不会因为刷新而变化 返回的是index\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(5, True,50)\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    kfold_X={}\n",
    "    kfold_Y={}\n",
    "    kfold_X,kfold_Y=X_train[train_index],X_test[test_index]\n",
    "#    print('训练集:{}'.format(train_index))\n",
    "#    print('测试集:{}'.format(test_index))\n",
    "#    print(kfold_X)\n",
    "#    print(kfold_Y)\n",
    "#    print(Y[train_index])\n",
    "#    print(Y[test_index])\n",
    "#    print(type(X_train[test_index]))\n",
    "    \n",
    "    #model.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[1:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-76d92a2eb9e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#在独立测试集上预测数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msvc_best_predict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvc_best\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdg_pca_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvc_best_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvc_best_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobA_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobB_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m             \u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             cache_size=self.cache_size)\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_sparse_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#在独立测试集上预测数据\n",
    "svc_best_predict=svc_best.predict(dg_pca_test)\n",
    "print(svc_best_predict)\n",
    "np.shape(svc_best_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                      n_jobs=None, oob_score=False, random_state=None,\n",
      "                      verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "8787 2197\n",
      "Fold 1\n",
      "8787 2197\n",
      "Fold 2\n",
      "8787 2197\n",
      "Fold 3\n",
      "8787 2197\n",
      "Fold 4\n",
      "8788 2196\n",
      "1 SVR(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0001,\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "Fold 0\n",
      "8787 2197\n",
      "Fold 1\n",
      "8787 2197\n",
      "Fold 2\n",
      "8787 2197\n",
      "Fold 3\n",
      "8787 2197\n",
      "Fold 4\n",
      "8788 2196\n",
      "2 XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
      "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "             silent=None, subsample=1, verbosity=1)\n",
      "Fold 0\n",
      "8787 2197\n",
      "Fold 1\n",
      "8787 2197\n",
      "Fold 2\n",
      "8787 2197\n",
      "Fold 3\n",
      "8787 2197\n",
      "Fold 4\n",
      "8788 2196\n"
     ]
    }
   ],
   "source": [
    "clfs = [RandomForestRegressor(n_estimators=500),\n",
    "        svm.SVR(kernel='rbf',C=10,gamma=0.0001),\n",
    "        XGBRegressor()]\n",
    "n_folds = 5\n",
    "kf = KFold(n_folds,True,50)\n",
    "skf=list(kf.split(y_train))\n",
    "\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    #'''依次训练各个单模型'''\n",
    "    print(j, clf)\n",
    "    dataset_blend_test_j = np.zeros((X_test.shape[0], len(skf)))#存目前这个模型上的测试集结果(之后求平均)\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        #'''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。'''\n",
    "        print(\"Fold\", i)\n",
    "        print(len(train),len(test))\n",
    "        #X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #y_submission =y_pred#1fold的预测结果\n",
    "        y_submission=np.arange(1,2198)\n",
    "        dataset_blend_train[test, j] = y_submission\n",
    "        #y_submission.reshape(len(y_submission),1)#一维数组转二维 说实话不知道需不需要 还是应该再研究一下\n",
    "        #\n",
    "        #dataset_blend_train[test, j] = #y_submission#在模型顺序对应的j位置 存1fold的预测结果\n",
    "        #dataset_blend_test_j[:, i] = clf.predict(X_predict)#存该模型该折下的测试集预测结果\n",
    "        \n",
    "    #'''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''\n",
    "    #dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)#测试集结果按行取平均后储存\n",
    "    \n",
    "    \n",
    "    #print(\"val auc Score: %f\" % r2_score(y_predict, dataset_blend_test[:, j]))\n",
    "    \n",
    "    #保存模型\n",
    "    #fl_name=\"stacking_{0}\".format(j)\n",
    "    #joblib.dump(clf, fl_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_blend_train = np.zeros((X_train.shape[0], len(clfs)))\n",
    "y_submission=np.arange(1,2198)\n",
    "y_submission.reshape(len(y_submission),1)\n",
    "j=np.arange(3,2200)\n",
    "dataset_blend_train[j,0] = y_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2007.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_blend_train[2009,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KFold' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-c9a26b104b4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mskf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mskf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'KFold' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "skf = KFold(n_folds,True,50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
