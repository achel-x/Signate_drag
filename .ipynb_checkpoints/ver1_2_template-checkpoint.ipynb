{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019/09/17 ver.1.1\n",
    "## 一、文件内容\n",
    "特征选择：GBRT <br />\n",
    "单模型构建：<br />\n",
    " 1：SVR<br />\n",
    " 2：RF<br />\n",
    " 3：XGBoost<br />\n",
    " 4：KNN<br />\n",
    " 5：GBRT<br />\n",
    "Stacking：<br />\n",
    " SVR默认\n",
    "## 二、数据集\n",
    "### 导入数据集变量<br />\n",
    "元数据集 训练集全部：dg_train →训练集 特征数据：dg_train<br />\n",
    "元数据集 测试集全部：dg_test  →测试集 特征数据：dg_test<br />\n",
    "<br />\n",
    "训练集 得分数据：score_train<br />\n",
    "测试集 序号：id_test<br />\n",
    "训练集 序号：id_train\n",
    "<br />\n",
    "### 处理数据集变量<br />\n",
    "标准化处理后的原训练集 特征数据：dg_scaled_train<br />\n",
    "标准化处理后的测试集合 特征数据：dg_scaled_test<br />\n",
    "### 特征选择结果\n",
    "特征选择后的训练集 特征数据：X<br />\n",
    "特征选择后的测试集 特征数据：X_predict<br />\n",
    "### 在X中继续划分数据<br />\n",
    "X_train：0.8比例的原训练集特征 用作训练和验证<br />\n",
    "X_test：0.2比例的原训练集特征 用作测试（模型选择）<br />\n",
    "y_train：0.8比例的原训练集分数 对应X_train<br />\n",
    "y_test：0.2比例的原训练集分数 对应X_test<br />\n",
    "\n",
    "## 三、模型信息\n",
    "\n",
    "\n",
    "变量名|模型名|超参数设置|随机数种子|训练集上预测值\n",
    ":---------:|:------------------:|:-----------------:|:-----------------:|:-------------------------:\n",
    "model_svc|支持向量回归||NA|\n",
    "model_svcRF|随机森林回归||160|\n",
    "model_xgbr|Xgboost回归||161|\n",
    "model_k_neighbor|KNN回归||NA|\n",
    "model_gradient_boosting_regressor|GBRT||162|\n",
    "\n",
    "## 四、随机数信息\n",
    "位置|使用函数|随机数种子\n",
    ":------:|:-------:|:--------:|\n",
    "特征处理|GBRT|20\n",
    "特征处理|train_test_split|21\n",
    "stacking|KFold|22\n",
    "预测|train_test_split|23\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、前処理（preprocess）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预设导入\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#机器学习导入\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import explained_variance_score \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_train=pd.read_csv('train.csv')\n",
    "dg_test=pd.read_csv(\"test.csv\")\n",
    "\n",
    "#单独提取ID列和score列\n",
    "id_train=dg_train[\"ID\"].values\n",
    "id_test=dg_test[\"ID\"].values\n",
    "score_train=dg_train[\"Score\"].values\n",
    "\n",
    "del dg_train[\"ID\"]\n",
    "del dg_train[\"Score\"]\n",
    "del dg_test[\"ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score 标准化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler().fit(dg_train)#标准化的mean var\n",
    "print(scaler)\n",
    "print(scaler.mean_)\n",
    "print(scaler.var_)\n",
    "dg_scaled_train=scaler.transform(dg_train)#标准化结果向量\n",
    "print(dg_scaled_train)\n",
    "np.shape(dg_scaled_train)\n",
    "\n",
    "dg_scaled_test=scaler.transform(dg_test)\n",
    "print(dg_scaled_test)\n",
    "np.shape(dg_scaled_test)#相同标准 标准化测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下开始特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基模型的特征选择\n",
    "clf_fs = ensemble.GradientBoostingRegressor(random_state=20)\n",
    "clf_fs = clf_fs.fit(dg_scaled_train, score_train)\n",
    "clf_fs.feature_importances_  \n",
    "\n",
    "model_fs = SelectFromModel(select_model, prefit=True)\n",
    "X = model_fs.transform(dg_scaled_train)\n",
    "\n",
    "\n",
    "#SelectFromModel(ensemble.GradientBoostingRegressor()).fit_transform(dg_scaled_train, score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict=model_fs.transform(dg_scaled_test)\n",
    "print(dg_scaled_test)\n",
    "np.shape(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.shape(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分数据集\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, score_train, test_size=0.2, random_state=21)\n",
    "X_train.shape,y_train.shape,X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、学習（train）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR模型实现\n",
    "model_svc=svm.SVR()\n",
    "model_svc.fit(X_train,y_train)\n",
    "\n",
    "print(model_svc.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_svc.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR模型调参数：C gamma\n",
    "param_g=[{\"C\": [1e0, 1e1, 1e2, 1e3],\"gamma\": np.logspace(-4, 0, 5)}]\n",
    "scores_g=['r2']\n",
    "for score in scores_g:\n",
    "    print(score)\n",
    "    clf=GridSearchCV(svm.SVR(),param_grid=param_g,cv=3,scoring=score)\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    print(clf.best_estimator_)\n",
    "    \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds=clf.cv_results_['std_test_score']\n",
    "    params = clf.cv_results_['params']\n",
    "    \n",
    "    for (param,mean_score,std_score) in zip(params,means,stds):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score,std_score,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR模型实现\n",
    "model_svc_t=clf.best_estimator_\n",
    "model_svc_t.fit(X_train,y_train)\n",
    "\n",
    "print(model_svc_t.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_svc_t.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "\n",
    "model_svc=clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svcRF=RandomForestRegressor(random_state=160)\n",
    "model_svcRF.fit(X_train,y_train)\n",
    "\n",
    "print(model_svcRF.score(X_test,y_test))#0.4697676007383277\n",
    "\n",
    "y_pred = model_svcRF.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_estimators=[{\"n_estimators\":[20,50,100,150,200,300,400,500]}]\n",
    "scores_g=['r2']\n",
    "for score in scores_g:\n",
    "    print(score)\n",
    "    clf=GridSearchCV(RandomForestRegressor(random_state=160),param_grid=N_estimators,cv=3,scoring=score)\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    print(clf.best_estimator_)\n",
    "    \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds=clf.cv_results_['std_test_score']\n",
    "    params = clf.cv_results_['params']\n",
    "    \n",
    "    for (param,mean_score,std_score) in zip(params,means,stds):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score,std_score,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svcRF_t=clf.best_estimator_\n",
    "model_svcRF_t.fit(X_train,y_train)\n",
    "\n",
    "print(model_svcRF_t.score(X_test,y_test))#默认0.4697676007383277\n",
    "\n",
    "y_pred = model_svcRF_t.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "model_svcRF=clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgbr = xgb.XGBRegressor(random_state=161)\n",
    "model_xgbr.fit(X_train, y_train)\n",
    "\n",
    "print(model_xgbr.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_xgbr.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调参 XGboost——n_estimators 其他默认\n",
    "cv_params = [{\"n_estimators\":[20,50,100,150,200,300,400,500]}]\n",
    "\n",
    "scores_GBM=['r2']\n",
    "\n",
    "for score in scores_GBM:\n",
    "    print(score)\n",
    "    model = XGBRegressor(random_state=161)\n",
    "    optimized_GBM=GridSearchCV(estimator=model, param_grid=cv_params, scoring=score, cv=3, verbose=1, n_jobs=-1)\n",
    "    optimized_GBM.fit(X_train,y_train)\n",
    "    \n",
    "    print(optimized_GBM.best_estimator_)\n",
    "    \n",
    "    means_GBM = optimized_GBM.cv_results_['mean_test_score']\n",
    "    stds_GBM=optimized_GBM.cv_results_['std_test_score']\n",
    "    params_GBM = optimized_GBM.cv_results_['params']\n",
    "    \n",
    "    for (param_GBM,mean_score_GBM,std_score_GBM) in zip(params_GBM,means_GBM,stds_GBM):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score_GBM,std_score_GBM,param_GBM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调参 XGboost——maxdepth minchildweight 其他默认\n",
    "cv_params = [{'max_depth': [3, 5,7,9], 'min_child_weight': [1, 3, 5]}]\n",
    "\n",
    "scores_GBM=['r2']\n",
    "\n",
    "for score in scores_GBM:\n",
    "    print(score)\n",
    "    model = optimized_GBM.best_estimator_\n",
    "    optimized_GBM=GridSearchCV(estimator=model, param_grid=cv_params, scoring=score, cv=3, verbose=1, n_jobs=-1)\n",
    "    optimized_GBM.fit(X_train,y_train)\n",
    "    \n",
    "    print(optimized_GBM.best_estimator_)\n",
    "    \n",
    "    means_GBM = optimized_GBM.cv_results_['mean_test_score']\n",
    "    stds_GBM=optimized_GBM.cv_results_['std_test_score']\n",
    "    params_GBM = optimized_GBM.cv_results_['params']\n",
    "    \n",
    "    for (param_GBM,mean_score_GBM,std_score_GBM) in zip(params_GBM,means_GBM,stds_GBM):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score_GBM,std_score_GBM,param_GBM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调参 XGboost——学习率\n",
    "cv_params = [{'learning_rate':  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]}]\n",
    "\n",
    "scores_GBM=['r2']\n",
    "\n",
    "for score in scores_GBM:\n",
    "    print(score)\n",
    "    model = optimized_GBM.best_estimator_\n",
    "    optimized_GBM=GridSearchCV(estimator=model, param_grid=cv_params, scoring=score, cv=3, verbose=1, n_jobs=-1)\n",
    "    optimized_GBM.fit(X_train,y_train)\n",
    "    \n",
    "    print(optimized_GBM.best_estimator_)\n",
    "    \n",
    "    means_GBM = optimized_GBM.cv_results_['mean_test_score']\n",
    "    stds_GBM=optimized_GBM.cv_results_['std_test_score']\n",
    "    params_GBM = optimized_GBM.cv_results_['params']\n",
    "    \n",
    "    for (param_GBM,mean_score_GBM,std_score_GBM) in zip(params_GBM,means_GBM,stds_GBM):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score_GBM,std_score_GBM,param_GBM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgbr_t = optimized_GBM.best_estimator_\n",
    "model_xgbr_t.fit(X_train, y_train)\n",
    "\n",
    "print(model_xgbr_t.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_xgbr_t.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "model_xgbr = optimized_GBM.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.kNN回归\n",
    "model_k_neighbor = neighbors.KNeighborsRegressor()\n",
    "model_k_neighbor.fit(X_train, y_train)\n",
    "\n",
    "print(model_k_neighbor.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_k_neighbor.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids=[\n",
    "    {\n",
    "        'weights':['uniform'],\n",
    "        'n_neighbors':[i for i in range(1,11)]\n",
    "    },\n",
    "    {\n",
    "        'weights':['distance'],\n",
    "        'n_neighbors':[i for i in range(1,11)],\n",
    "        'p':[i for i in range(1,6)]\n",
    "    }]\n",
    "scores_KNN=['r2']\n",
    "\n",
    "for score in scores_KNN:\n",
    "    print(score)\n",
    "    model = neighbors.KNeighborsRegressor()\n",
    "    clf_k=GridSearchCV(estimator=model, param_grid=param_grids, scoring=score, cv=3, verbose=1, n_jobs=-1)\n",
    "    clf_k.fit(X_train,y_train)\n",
    "    \n",
    "    print(clf_k.best_estimator_)\n",
    "    \n",
    "    means_KNN = clf_k.cv_results_['mean_test_score']\n",
    "    stds_KNN=clf_k.cv_results_['std_test_score']\n",
    "    params_KNN = clf_k.cv_results_['params']\n",
    "    \n",
    "    for (param_KNN,mean_score_KNN,std_score_KNN) in zip(params_KNN,means_KNN,stds_KNN):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score_KNN,std_score_KNN,param_KNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k_neighbor_t = clf_k.best_estimator_\n",
    "model_k_neighbor_t.fit(X_train, y_train)\n",
    "\n",
    "print(model_k_neighbor_t.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_k_neighbor_t.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "\n",
    "model_k_neighbor = clf_k.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gradient_boosting_regressor = ensemble.GradientBoostingRegressor(random_state=162)  \n",
    "\n",
    "model_gradient_boosting_regressor.fit(X_train, y_train)\n",
    "\n",
    "print(model_gradient_boosting_regressor.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_gradient_boosting_regressor.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_estimators=[{\"n_estimators\":[20,50,100,150,200,300,400,500]}]\n",
    "scores_g=['r2']\n",
    "for score in scores_g:\n",
    "    print(score)\n",
    "    clf_gb=GridSearchCV(ensemble.GradientBoostingRegressor(random_state=162),param_grid=N_estimators,cv=3,scoring=score)\n",
    "    clf_gb.fit(X_train,y_train)\n",
    "    \n",
    "    print(clf_gb.best_estimator_)\n",
    "    \n",
    "    means = clf_gb.cv_results_['mean_test_score']\n",
    "    stds=clf_gb.cv_results_['std_test_score']\n",
    "    params = clf_gb.cv_results_['params']\n",
    "    \n",
    "    for (param,mean_score,std_score) in zip(params,means,stds):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score,std_score,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gradient_boosting_regressor_t = clf_gb.best_estimator_\n",
    "\n",
    "model_gradient_boosting_regressor_t.fit(X_train, y_train)\n",
    "\n",
    "print(model_gradient_boosting_regressor_t.score(X_test,y_test))\n",
    "\n",
    "y_pred = model_gradient_boosting_regressor_t.predict(X_test)\n",
    "print(r2_score(y_test,y_pred, multioutput='variance_weighted'))\n",
    "\n",
    "model_gradient_boosting_regressor = clf_gb.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二阶段stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''创建训练的数据集'''\n",
    "#data, target = make_blobs(n_samples=50000, centers=2, random_state=0, cluster_std=0.60)\n",
    " \n",
    "# '''模型融合中使用到的各个单模型'''\n",
    "clfs = [model_svc,\n",
    "        model_svcRF,\n",
    "        model_xgbr,\n",
    "        model_k_neighbor,\n",
    "        model_gradient_boosting_regressor\n",
    "       ]\n",
    " \n",
    "#'''切分一部分数据作为测试集'''\n",
    "X=X#训练集 数据\n",
    "X_predict=X_predict#测试集 数据\n",
    "y=score_train#训练集 分数\n",
    "#y_predict = \n",
    "\n",
    "\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))#第一轮 保存各个模型在训练集上的预测结果 训练集合个数×模型数\n",
    "dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))#第一轮 保存各个模型在测试集上的预测结果 训练集合个数×模型数\n",
    "\n",
    "#'''5折stacking'''\n",
    "n_folds = 5\n",
    "kf = KFold(n_folds,True,22)\n",
    "skf=list(kf.split(X))#X或者y\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    #'''依次训练各个单模型'''\n",
    "    print(j, clf)\n",
    "    dataset_blend_test_j = np.zeros((X_predict.shape[0], len(skf)))#存目前这个模型上的测试集结果(之后求平均)\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        #'''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。'''\n",
    "        print(\"Fold\", i)\n",
    "        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict(X_test)#1fold的预测结果\n",
    "        y_submission=y_submission.flatten()#2维数组转1维 KNN需要 \n",
    "        #y_submission.reshape(len(y_submission),1)#一维数组转二维 可以不加\n",
    "        \n",
    "        dataset_blend_train[test, j] = y_submission#在模型顺序对应的j位置 存1fold的预测结果\n",
    "        dataset_blend_test_j[:, i] = clf.predict(X_predict).flatten()#存该模型该折下的测试集预测结果\n",
    "        \n",
    "    #'''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)#测试集结果按行取平均后储存\n",
    "    \n",
    "    \n",
    "    #print(\"val auc Score: %f\" % r2_score(y_predict, dataset_blend_test[:, j]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导出数据集到本地\n",
    "submission_train_1=pd.DataFrame(dataset_blend_train)\n",
    "#submission_train_1.head()\n",
    "submission_train_1.to_csv('dataset_blend_train.csv',index=False)#第一轮训练后 train集合预测得到的score集合 训练集样本数x3个模型\n",
    "\n",
    "submission_test_1=pd.DataFrame(dataset_blend_test)\n",
    "#submission_test_1.head()\n",
    "submission_test_1.to_csv('dataset_blend_test.csv',index=False)#第一轮训练后 test集合预测得到的score集合 测试机样本数x3个模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、予測（predict）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第二轮 \n",
    "model_stacking_svr = svm.SVR()\n",
    "\n",
    "model_stacking_svr.fit(dataset_blend_train, y)\n",
    "y_submission = model_stacking_svr.predict(dataset_blend_test)\n",
    "\n",
    "#生成文件\n",
    "submission_df=pd.DataFrame(data={'Id':id_test,'SalePrice':y_submission})\n",
    "submission_df.head()\n",
    "submission_df.to_csv('baseline_st2_test1.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调参数\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_blend_train, y, test_size=0.2, random_state=123)\n",
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)\n",
    "\n",
    "#SVR模型调参数：C gamma\n",
    "param_g=[{\"C\": [1e0, 1e1, 1e2, 1e3],\"gamma\": np.logspace(-4, 0, 5)}]\n",
    "scores_g=['r2']\n",
    "for score in scores_g:\n",
    "    print(score)\n",
    "    clf_st=GridSearchCV(svm.SVR(),param_grid=param_g,cv=3,scoring=score)\n",
    "    clf_st.fit(X_train,y_train)\n",
    "    \n",
    "    print(clf_st.best_estimator_)\n",
    "    \n",
    "    means = clf_st.cv_results_['mean_test_score']\n",
    "    stds=clf_st.cv_results_['std_test_score']\n",
    "    params = clf_st.cv_results_['params']\n",
    "    \n",
    "    for (param,mean_score,std_score) in zip(params,means,stds):\n",
    "        print(\"%0.3f(+/-%0.03f) for %r\"%(mean_score,std_score,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_st_best = clf.best_estimator_\n",
    "\n",
    "clf_st_best.fit(dataset_blend_train, y)\n",
    "y_submission = clf_st_best.predict(dataset_blend_test)\n",
    "\n",
    "#生成文件\n",
    "submission_df=pd.DataFrame(data={'Id':id_test,'SalePrice':y_submission})\n",
    "submission_df.head()\n",
    "submission_df.to_csv('baseline_st2_test2.csv',header=False,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
